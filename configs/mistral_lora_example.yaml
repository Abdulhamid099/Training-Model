# Example configuration for LoRA fine-tuning
base_model_name_or_path: mistralai/Mistral-7B-Instruct-v0.2
use_4bit: true
bnb_4bit_compute_dtype: float16

# Data
train_jsonl_path: data/sample.jsonl
text_template: "<s>[INST] {prompt} [/INST] {response}</s>"

# Training
output_dir: outputs/mistral7b-lora-demo
num_train_epochs: 1
per_device_train_batch_size: 1
gradient_accumulation_steps: 4
learning_rate: 2.0e-4
weight_decay: 0.0
warmup_ratio: 0.03
lr_scheduler_type: cosine
logging_steps: 10
save_steps: 50
save_total_limit: 1
bf16: false
fp16: true
seed: 42

# LoRA
lora_r: 16
lora_alpha: 32
lora_dropout: 0.05
target_modules:
  - q_proj
  - k_proj
  - v_proj
  - o_proj
  - gate_proj
  - up_proj
  - down_proj